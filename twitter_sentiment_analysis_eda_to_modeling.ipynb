{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiQe4PbwcwC1"
      },
      "source": [
        "# Twitter Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoTi4nn3cwC7"
      },
      "source": [
        "![Example of sentiment analysis](https://media-exp1.licdn.com/dms/image/C5612AQERP5yD4Ov6Fw/article-cover_image-shrink_600_2000/0?e=1610582400&v=beta&t=O99Hkcjllunfb-MsfL_ANv5dYlTpsZobRxIE-eqdUiw)\n",
        "Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
        "A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level—whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. Advanced, \"beyond polarity\" sentiment classification looks, for instance, at emotional states such as \"angry\", \"sad\", and \"happy\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdxAKZRrcrYD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3tx2zCfcoLI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os6pmg8PcwC7"
      },
      "source": [
        "### About this Notebook\n",
        "\n",
        "A Data Science project has 5 major stages in its life cycle:\n",
        "1. Data Collection\n",
        "2. Data Processing\n",
        "3. Exploratory Analysis of the Data\n",
        "4. Data Modeling\n",
        "5. Interpreting the Data\n",
        "\n",
        "This notebook will help create an understanding of how useful interpretation could be made of real world data using various data science principles.\n",
        "\n",
        "This notebook will make use of popular libraries such as Plotly for visualizations. \n",
        "\n",
        "Pyspark would then be used to handle the big Data. Modeling would be done along with TF-IDF and Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6HQElQ6cwC8"
      },
      "source": [
        "### Contents\n",
        "\n",
        "1. Data Collection\n",
        "2. EDA\n",
        "3. Processing the Data\n",
        "4. Modeling\n",
        "5. Interpretaion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA98dNcncwC9"
      },
      "source": [
        "**0. Necessary imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc-_Rwe_cwC9",
        "outputId": "8ef61173-232f-496f-d2ba-065ecd464c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 8.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.3\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 27 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 53.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=2a4e163f9d6219116e098ce435e54f6bc04dd3bde668ca1f1617c83a487bf09b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# some external packages\n",
        "!pip install pyspellchecker\n",
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r03-hvMgcwC_"
      },
      "outputs": [],
      "source": [
        "#### for data manipulation and math operations ####\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#### for visualizations ####\n",
        "# plotly\n",
        "from plotly.offline import iplot\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "#### NLP packages ####\n",
        "# NLTK library\n",
        "from nltk.corpus import stopwords\n",
        "# SKLearn \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# py-spell checker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "\n",
        "#### other useful packages ####\n",
        "import string\n",
        "from collections import Counter\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "#### Pyspark packages ####\n",
        "import findspark\n",
        "# findspark.init()\n",
        "import pyspark as ps\n",
        "import warnings\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qKItzC1bfNEy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NlMFEuifNlr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxxgXbkqfVMc"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"/content/drive/My Drive/\"\n",
        "\n",
        "with ZipFile(\"/content/drive/My Drive/archive.zip\", 'r') as ziped:\n",
        "  ziped.extractall()\n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x35aYoHwcwDA"
      },
      "source": [
        "\n",
        "## I. Data Collection\n",
        "\n",
        "Dataset being used is the \"<a href=\"https://www.kaggle.com/kazanova/sentiment140\">**Sentiment140 dataset with 1.6 million tweets**</a>\", which is a publicly available dataset on Kaggle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P66cOxAXcwDB"
      },
      "source": [
        "**Reading the Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny0cP1SwcwDD"
      },
      "outputs": [],
      "source": [
        "file_path = 'training.1600000.processed.noemoticon.csv'\n",
        "colnames=['sentiment', 'ids', 'date', 'flag','user','text'] \n",
        "train = pd.read_csv(file_path,encoding = \"ISO-8859-1\", header=None, names=colnames) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWaSNNl9cwDE"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv3mriDvcwDE"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape of training data: {train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm2JzWNXcwDF"
      },
      "outputs": [],
      "source": [
        "train = train[['text','sentiment']]\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1EuxKsPcwDG"
      },
      "outputs": [],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qob7-uHcwDH"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNMNUhFWcwDI"
      },
      "outputs": [],
      "source": [
        "# Lets use a subset of the data for faster processing\n",
        "# Lets use about 100K rows of data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train['text'], train['sentiment'], test_size=0.85, random_state=42)\n",
        "train = pd.concat([X_train,y_train],axis=1)\n",
        "print(len(train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNV29TmrcwDI"
      },
      "source": [
        "## II. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwaGv2J5cwDI"
      },
      "source": [
        "Sentiment is the class label that would have to be predicted.\n",
        "\n",
        "Understanding the distribution of data as per the classes is of high importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM2Nr0nocwDJ"
      },
      "outputs": [],
      "source": [
        "# lets run a groupby query which has similar functionalities to dealing with RDBMS\n",
        "class_group_counts = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\n",
        "class_group_counts.style.background_gradient(cmap='Blues')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu6olVtFcwDJ"
      },
      "source": [
        "Bar Chart for Distribution of Data in accordance to Sentiment classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFOMdncRcwDJ"
      },
      "outputs": [],
      "source": [
        "# create a trace\n",
        "trace = go.Bar(\n",
        "    x = class_group_counts.sentiment,\n",
        "    y = class_group_counts.text,\n",
        "    name = 'Data Frequency',\n",
        "    marker={'color': ['#e57373','#f06292','#ba68c8']}\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "layout = go.Layout(title=\"Distribution of the categorical classes\")\n",
        "\n",
        "fig = go.Figure(data = data,layout=layout)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5JFNQRhcwDK"
      },
      "source": [
        "Pie Chart for Distribution of Data in accordance to Sentiment classes.\n",
        "\n",
        "Helps for easier readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERyHqQTycwDK"
      },
      "outputs": [],
      "source": [
        "# create the trace\n",
        "trace = go.Pie(\n",
        "    labels = class_group_counts.sentiment,\n",
        "    values = class_group_counts.text\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "layout = go.Layout(title=\"Pie plot of the distribution of the categorical classes\")\n",
        "\n",
        "fig = go.Figure(data = data,layout=layout)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5-zORPdcwDK"
      },
      "source": [
        "Lets count the frequency of words of the respective classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFY27izLcwDL"
      },
      "outputs": [],
      "source": [
        "# lists to keep track of word-frequencyies for the two classes\n",
        "positive_words_count = []\n",
        "negative_words_count = []\n",
        "\n",
        "for i in tqdm(range(len(train))):\n",
        "    if train.iloc[i]['sentiment'] == 4:\n",
        "        positive_words_count.append(len(train.iloc[i]['text'].split()))\n",
        "    elif train.iloc[i]['sentiment'] == 0:\n",
        "        negative_words_count.append(len(train.iloc[i]['text'].split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WRfFug4cwDL"
      },
      "outputs": [],
      "source": [
        "# plot the histogram\n",
        "\n",
        "trace2 = go.Histogram(\n",
        "    x = np.array(positive_words_count), \n",
        "    name = 'Positive'\n",
        ")\n",
        "trace3 = go.Histogram(\n",
        "    x = np.array(negative_words_count),\n",
        "    name = 'Negative'\n",
        ")\n",
        "\n",
        "data = [trace2,trace3]\n",
        "\n",
        "layout = go.Layout(\n",
        "    barmode='overlay',\n",
        "    title=\"Word-Frequencies of each class\")\n",
        "\n",
        "fig = go.Figure(data = data, layout = layout)\n",
        "fig.update_traces(opacity=0.6)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDg64ZmkcwDL"
      },
      "source": [
        "**Most Common Words in the Tweets**\n",
        "\n",
        "This would help us understand which words appears several times in the tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rXSY3VjcwDL"
      },
      "outputs": [],
      "source": [
        "# lets now use pythons counter module to create word-frequency dictionary\n",
        "# the key would be the word while the value would be the words count\n",
        "\n",
        "positive_words_count = Counter()\n",
        "negative_words_count = Counter()\n",
        "\n",
        "counts = {4:positive_words_count,0:negative_words_count}\n",
        "\n",
        "# iterate over every data row\n",
        "for i in tqdm(range(len(train))):\n",
        "    sentiment_class = train.iloc[i]['sentiment']\n",
        "    for word in train.iloc[i]['text'].split():\n",
        "        if word in counts[sentiment_class]:\n",
        "            counts[sentiment_class][word] += 1\n",
        "        else:\n",
        "            counts[sentiment_class][word] = 1\n",
        "            \n",
        "top_words_positive_count = sorted(positive_words_count.items(), key = lambda x: x[1],reverse=True)[:20]\n",
        "top_words_negative_count = sorted(negative_words_count.items(), key = lambda x: x[1],reverse=True)[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQtzjp7BcwDM"
      },
      "outputs": [],
      "source": [
        "# Creating subplots for the most common words in each class\n",
        "fig = make_subplots(rows=2, cols=1,\n",
        "                    subplot_titles=(\n",
        "                        \"Positive Tweets\",\n",
        "                        \"Negative Tweets\")\n",
        ")\n",
        "                    \n",
        "\n",
        "# trace for positive class\n",
        "x,y = zip(*top_words_positive_count)\n",
        "fig.append_trace(\n",
        "    go.Bar(\n",
        "        x = x,\n",
        "        y = y ,\n",
        "        name = 'Positive'),\n",
        "    row=1, col=1)\n",
        "\n",
        "# trace for negative class\n",
        "x,y = zip(*top_words_negative_count)\n",
        "fig.append_trace(\n",
        "    go.Bar(\n",
        "        x = x,\n",
        "        y = y ,\n",
        "        name = 'Negative'),\n",
        "    row=2, col=1)\n",
        "\n",
        "\n",
        "fig.update_layout(height=500, width=900, title_text=\"Top words in different classes\",showlegend=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSfuSuVscwDM"
      },
      "source": [
        "From the above stats its pretty evident that the top words in each class are common spoken english words and don't contribute much towards understanding the sentiment of the tweet as they appear in all classes. \n",
        "\n",
        "These words are also called as stop words and could be eliminated from the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7SQ6HCUcwDM"
      },
      "source": [
        "**Analyzing punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC5G9u4EcwDM"
      },
      "outputs": [],
      "source": [
        "punct = Counter()\n",
        "\n",
        "for text in train['text']:\n",
        "    for word in text:\n",
        "        if word in string.punctuation:\n",
        "            if word in punct:\n",
        "                punct[word] += 1\n",
        "            else:\n",
        "                punct[word] = 1\n",
        "\n",
        "# sort the punctuations frequencies\n",
        "top_punct_count = sorted(punct.items(),key=lambda x:x[1],reverse=True)[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74Tv33RpcwDN"
      },
      "outputs": [],
      "source": [
        "x, y = zip(*top_punct_count)\n",
        "trace = go.Bar(\n",
        "    x = x,\n",
        "    y = y,\n",
        ")\n",
        "\n",
        "data = [trace]\n",
        "layout = go.Layout(title=\"Most Frequent Puntuations\",height=300, width=900,)\n",
        "fig = go.Figure(data=data,layout=layout)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBkas7Y3cwDN"
      },
      "source": [
        "**N-Gram Analysis**\n",
        "\n",
        "All the analysis done earlier were done on unigrams(on the basis of a single word). Lets now check what are the most common Bi-Grams and Tri-Grams.\n",
        "\n",
        "\"*I love to eat pizza.*\"\n",
        "\n",
        "A 1-gram (or unigram) is a one-word sequence. For the above sentence, the unigrams would simply be: “I”, “love”, “to”, “eat”, “pizza”.\n",
        "\n",
        "A 2-gram (or bigram) is a two-word sequence of words, like “I love”, “love to”, or “to eat”, \"eat pizza\". And a 3-gram (or trigram) is a three-word sequence of words like “I love to”, “love to eat” or “to eat pizza”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drCISnk7cwDN"
      },
      "outputs": [],
      "source": [
        "# Using the CountVectorizer of the sklearn library for n-gram analysis\n",
        "def get_top_n_grams(corpus,N=2, n=20):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        corpus: list of text data\n",
        "        N : N-gram \n",
        "        n : top n N-grams\n",
        "    returns:\n",
        "        Returns the n most common N-grams\n",
        "    \"\"\"\n",
        "    \n",
        "    # create the CountVectorizer object\n",
        "    vec = CountVectorizer(ngram_range=(N,N))\n",
        "    # fit it on the corpus\n",
        "    bag_of_words = vec.fit_transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word,sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x:x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB4ZfnIncwDN"
      },
      "outputs": [],
      "source": [
        "bi_grams = get_top_n_grams(train['text'],N=2)\n",
        "tri_grams = get_top_n_grams(train['text'],N=3)\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=2,cols=1,\n",
        "    subplot_titles=(\"Bi-grams\",\"Tri-grams\")   \n",
        ")\n",
        "\n",
        "x,y = zip(*bi_grams)\n",
        "fig.append_trace(\n",
        "    go.Bar(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        name='bi-gram'\n",
        "    ),\n",
        "    row=1,col=1\n",
        ")\n",
        "\n",
        "x,y = zip(*tri_grams)\n",
        "fig.append_trace(\n",
        "    go.Bar(\n",
        "        x=x,\n",
        "        y=y,\n",
        "        name='tri-gram'\n",
        "    ),\n",
        "    row=2,col=1\n",
        ")\n",
        "\n",
        "fig.update_layout(title=\"Most common N-grams\",height=900,width=900,)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JW-YDJVcwDO"
      },
      "source": [
        "## III. Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCFXFXmgcwDO"
      },
      "source": [
        "From the above analysis it is certain that a lot of work has to be done on cleaning the data.\n",
        "\n",
        "Major components of processing text data include:\n",
        "Elimination of ...\n",
        "1. punctuations\n",
        "2. urls\n",
        "3. emojis\n",
        "4. stop words.\n",
        "5. HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4WDwMo9cwDO"
      },
      "source": [
        "**Removing HTML-tags**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMi5p7nucwDO"
      },
      "outputs": [],
      "source": [
        "def remove_HTML(text):\n",
        "    \"\"\"\n",
        "    Inputs a string and outputs a string free of any HTML tags\n",
        "    \"\"\"\n",
        "    tag = re.compile(r'<.*?>')\n",
        "    \n",
        "    return tag.sub(r'',text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uYkllc7cwDO"
      },
      "source": [
        "Lets test the above function with an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyPE6EawcwDP"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"<div>\n",
        "<h1>Pizzeria</h1>\n",
        "<p>Best pizza in town</p>\n",
        "<a href=\"https://pizzeria.com\">getting started</a>\n",
        "</div>\"\"\"\n",
        "\n",
        "print(remove_HTML(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMT1WPocwDP"
      },
      "source": [
        "**Removing URLs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zts6hjnZcwDP"
      },
      "outputs": [],
      "source": [
        "def remove_URL(text):\n",
        "    \"\"\"\n",
        "    Inputs a string and outputs a string free of any URLs\n",
        "    \"\"\"\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    \n",
        "    return url.sub(r'',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPs21RppcwDP"
      },
      "outputs": [],
      "source": [
        "text = \"New Pizza :https://pizzeria.com-getting-started you will love it\"\n",
        "remove_URL(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V715mNxkcwDP"
      },
      "source": [
        "**Removing Emojis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC3_TEtCcwDP"
      },
      "outputs": [],
      "source": [
        "def remove_emojis(text):\n",
        "    \"\"\"\n",
        "    Inputs a string and outputs a string free of any emojis\n",
        "    \"\"\"\n",
        "    emoji = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "    \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    return emoji.sub(r'',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgD_tveocwDQ"
      },
      "outputs": [],
      "source": [
        "text = \"I didn't like the pizza 😔😔\"\n",
        "remove_emojis(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqBxekhJcwDQ"
      },
      "source": [
        "**Removing Punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vN2IE1ybcwDQ"
      },
      "outputs": [],
      "source": [
        "def remove_punctuations(text):\n",
        "    \"\"\"\n",
        "    Inputs a string and outputs a string free of any punctuations\n",
        "    \"\"\"\n",
        "    punct = re.compile(r'[^\\w\\s]')\n",
        "    \n",
        "    return punct.sub(r'',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL5DsjPMcwDQ"
      },
      "outputs": [],
      "source": [
        "s = \"string. With. #$@#$Punctuation?\"\n",
        "remove_punctuations(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmq285OucwDQ"
      },
      "source": [
        "**Spell-checker and correction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5zdzxWYcwDR"
      },
      "outputs": [],
      "source": [
        "def correct_typo(text):\n",
        "    spell = SpellChecker()\n",
        "    correct = []\n",
        "    # find the wrongly spelled words\n",
        "    misspelled = spell.unknown(text.split())\n",
        "    \n",
        "    for word in text.split():\n",
        "        # if the word is misspelled then correct it\n",
        "        if word in misspelled:\n",
        "            correct.append(spell.correction(word))\n",
        "        else:\n",
        "            correct.append(word)\n",
        "            \n",
        "    return \" \".join(correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYji9X0ecwDR"
      },
      "outputs": [],
      "source": [
        "# testing the correct_typo function\n",
        "text = \"I love the pizz at Jimmy's, it's simpl fanastic\"\n",
        "correct_typo(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IxuYXjmcwDR"
      },
      "source": [
        "**Remove Stop words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR93iMgjcwDR"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# set of all stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "stop.remove('not') # exclude not\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    \"\"\"\n",
        "    inputs a text string and outputs a string without any stopwords\n",
        "    \"\"\"\n",
        "    sentence = [] # list without any stopwords\n",
        "    for word in text.split():\n",
        "        if word not in stop:\n",
        "            sentence.append(word)\n",
        "            \n",
        "    return \" \".join(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQcVK3NQcwDR"
      },
      "outputs": [],
      "source": [
        "# testing the elimination of stop words function\n",
        "text = \"I dislike the fried chicken, but crave for the Lasanga\"\n",
        "remove_stop_words(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_InZQvNcwDS"
      },
      "source": [
        "**Lets now assemble all of the above functions to return a cleaned text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fmP4pZAcwDS"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    inputs a string:\n",
        "    -------------------------------------\n",
        "    outputs a string free from \n",
        "    1) html-tags\n",
        "    2) urls\n",
        "    3) emojis\n",
        "    4) emojis\n",
        "    5) stopwords\n",
        "    and lastly corrects the misspelled words\n",
        "    \"\"\"\n",
        "    text = remove_HTML(text)\n",
        "    text = remove_URL(text)\n",
        "    text = remove_emojis(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = remove_stop_words(text)\n",
        "    text = correct_typo(text)\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psAxEg1BcwDS"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"<div>\n",
        "<h1>Pizzeria</h1>\n",
        "<p>Best pizza in town</p>\n",
        "<a href=\"https://pizzeria.com\">getting started</a>\n",
        "</div> Follow the link at https://pizzeria.com. But the pizz is not that great!!! 😔😔! Disapointed\"\"\"\n",
        "\n",
        "clean_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm_GJuCMcwDS"
      },
      "outputs": [],
      "source": [
        "print(len(train))\n",
        "corpus = []\n",
        "sentimental=[]\n",
        "for i in tqdm(range(len(train[10000:60000]))):\n",
        "    text = train.iloc[i]['text']\n",
        "    sentimental.append(train.iloc[i]['sentiment'])\n",
        "    corpus.append(clean_text(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4H7n6tTcwDS"
      },
      "outputs": [],
      "source": [
        "print(len(corpus),\n",
        "len(sentimental))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILBQVmczcwDT"
      },
      "source": [
        "## IV. Data Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fupQt2AxcwDT"
      },
      "source": [
        "**Creating a Spark Context to initiate a connection to a cluster to obtain the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQbgnxF256Vx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN24X5x3cwDT"
      },
      "outputs": [],
      "source": [
        "sc = ps.sql.SparkSession.builder.getOrCreate()  \n",
        "sqlContext = sc\n",
        "print(\"Just created a SparkContext\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69vcViZMrmQs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWsoqZVw667u"
      },
      "outputs": [],
      "source": [
        "print(len(train[:]))\n",
        "percentile_list = pd.DataFrame({'tweet' : corpus,\n",
        "                                'target' : sentimental }, \n",
        "                                columns=['tweet','target'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX0MrPq8A00m"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/cleaned40_train.csv'\n",
        "\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "  percentile_list.to_csv(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z_o_AWQ1Uqb"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "file=files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6ZeHvXWcwDT"
      },
      "source": [
        "Loading the cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsMV97VgcwDT"
      },
      "outputs": [],
      "source": [
        "file_path = 'cleaned24_train - cleaned23_train.csv'\n",
        "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(file_path)\n",
        "type(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmL-kdOCcwDT"
      },
      "outputs": [],
      "source": [
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHM-p0LUcwDU"
      },
      "outputs": [],
      "source": [
        "# lets view the size of the data\n",
        "print(f\"Size of the data = {df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up2MzWrFcwDU"
      },
      "outputs": [],
      "source": [
        "# Lets split the data for training and testing the model\n",
        "(train_set, val_set, test_set) = df.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iutVeWkcwDU"
      },
      "source": [
        "**Feature Extraction with TF-IDF**\n",
        "\n",
        "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
        "\n",
        "It has many uses, most importantly in automated text analysis, and is very useful for scoring words in machine learning algorithms for Natural Language Processing (NLP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PV25nqLcwDU"
      },
      "outputs": [],
      "source": [
        "# Creating the pipeline for feature extraction\n",
        "\n",
        "# tokenizing the data\n",
        "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
        "\n",
        "# Creating an instance of the TF-IDF\n",
        "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
        "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "\n",
        "# to convert string target to index target\n",
        "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "\n",
        "# the complete pipeline: sequence of various stages\n",
        "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZ5t7bQcwDU"
      },
      "source": [
        "**Extract Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL-bN6U1cwDU"
      },
      "outputs": [],
      "source": [
        "train_set=train_set.na.drop()\n",
        "pipelineFit = pipeline.fit(train_set)\n",
        "train_df = pipelineFit.transform(train_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GFYq3oCcwDV"
      },
      "source": [
        "**Extracting features of the validation set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lq9dm-LcwDV"
      },
      "outputs": [],
      "source": [
        "val_df = pipelineFit.transform(val_set)\n",
        "train_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUlShL-CcwDV"
      },
      "source": [
        "**Modeling with Logistic Regression**\n",
        "\n",
        "Lets now apply logistic regression to the data as we now have the extracted features of every data point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nzwPeujcwDV"
      },
      "outputs": [],
      "source": [
        "LR = LogisticRegression()\n",
        "model = LR.fit(train_df)\n",
        "predictions = model.transform(val_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko01XVTCcwDV"
      },
      "source": [
        "**Evaluation Metrics for the model (TF-IDF with Logistic Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYRMjqm6cwDV"
      },
      "outputs": [],
      "source": [
        "print(predictions[7])\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er8q6bZEcwDW"
      },
      "source": [
        "## V. Data Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5oQngVecwDW"
      },
      "outputs": [],
      "source": [
        "test = {\n",
        "    'tweet':[\n",
        "        'OMG! I\"m so sick of the US elections and the corruptions',\n",
        "'I love the Master Chef US, its streaming this Friday on Fox #masterchef'\n",
        "        \n",
        "    ],\n",
        "    'target':[0,1]\n",
        "}\n",
        "\n",
        "test_ = pd.DataFrame(test)\n",
        "\n",
        "test_ = sqlContext.createDataFrame(test_)\n",
        "print(test_set['tweet'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Eu8E-9ccwDW"
      },
      "outputs": [],
      "source": [
        "def model_predict(test_):\n",
        "    features = pipelineFit.transform(test_)\n",
        "    preds = model.transform(features)\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOhpNikJcwDW"
      },
      "outputs": [],
      "source": [
        "pred = model_predict(test_)\n",
        "pred.select('prediction').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7bhQ1PLcwDW"
      },
      "source": [
        "### Acknowledgements\n",
        "\n",
        "* <a href=\"https://en.wikipedia.org/wiki/Sentiment_analysis#:~:text=Sentiment%20analysis%20(also%20known%20as,affective%20states%20and%20subjective%20information.\">Sentiment Analysis Wikipedia</a>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "H7bhQ1PLcwDW"
      ],
      "name": "twitter-sentiment-analysis-eda-to-modeling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 565.752919,
      "end_time": "2020-12-02T19:06:48.150255",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-12-02T18:57:22.397336",
      "version": "2.1.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}